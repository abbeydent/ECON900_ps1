# ECON900_ps1

This respository contains several .py files that each perform a seperate task which together work to scrape data from
coinmarketcap.com along with one used to conduct some machine learning analysis on the data retrieved from the scrapper.
The data is on cryptocurrencies and for the top 500 cryptocurrencies the historical data will also be scrapped. 
For simplicity, two seperate parsers are used, one for the regualar html files and one for the historical html files.



#coinmarket_request.py

```
import urllib.request
import os
import time
import datetime
from bs4 import BeautifulSoup
import pandas as pd 
```
First the libraries required to run this file are imported


```
url = 'https://coinmarketcap.com/all/views/all/'
```
This defines the url that will be used to request the html files from coinmarketcap.com for all the currencies. 


```
for i in range(30):
	current_time_stamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d%H%M%S')
	print("requesting " + url + current_time_stamp)
	f = open("html_files/coinmarketcap" + current_time_stamp +".html", "wb")
	response = urllib.request.urlopen('https://coinmarketcap.com/all/views/all/')
	html = response.read()
	#print(html)
	f.write(html)
	f.close()
	time.sleep(21600)

```
The first for loop request the html code from coinmarketcap.com for all currencies. The range is set to 30 and the sleep
time is set to 21600 so that the program will request the files every 6 hours over a one week period. 
 
 
```
f = open("html_files/coinmarketcap.html", "r")
soup = BeautifulSoup(f.read(), features='lxml')
f.close()
currencies_table = soup.find("table", {"id": "currencies-all"})
currencies_tbody = currencies_table.find("tbody")
currency_rows = currencies_tbody.find_all("tr")
crypto_list = []
crypto_name_list = []
for r in currency_rows:
	currency_short_name = r.find("td", {"class": "currency-name"}).find("span", {"class": "currency-symbol"}).find("a").text
	currency_name = r.find("td", {"class": "currency-name"}).find("a",{"class":"currency-name-container"}).text
	currency_link = r.find("td", {"class": "currency-name"}).find("a", {"class": "currency-name-container"})['href']
	crypto_list.append(currency_link)
	crypto_name_list.append(currency_short_name)
	#print(crypto_list)
```
This code chunk creates two lists, one containing the currency short names and one containing the hrefs for each currency
symbol that will allow is to create a loop to get the historical data for each crypto currency. This is done by using an
html file generated by running the first for loop one time and without adding the time stamp, and retrieving the data needed
from that html_file.


```
for i in range(500):
	print("requesting " + crypto_name_list[i])
	f = open("historical_html_files/" + crypto_name_list[i]  + ".html", "wb")
	# print("https://coinmarketcap.com" + crypto_list[i]  + "historical-data" + "?start=20130428&end=20190422")
	response = urllib.request.urlopen("https://coinmarketcap.com" + crypto_list[i]  + "historical-data" + "?start=20130428&end=20190422")
	html = response.read()
	f.write(html)
	f.close()
	time.sleep(30)
```
This code chunk creates a for loop that loops through each of the top 500 currencies and requests for each currency the 
historical data from the past year with the sleep time set to 30 seconds. 



#coinmarket_parser.py

```
from bs4 import BeautifulSoup
import os
import glob
import pandas as pd
```
First the libraries needed for this program are imported


```
for one_file_name in glob.glob("html_files/*.html"):
	print("parsing " + one_file_name)
	scrapping_time = os.path.splitext(os.path.basename(one_file_name))[0].replace("coinmarketcap","")
	f = open(one_file_name, "r")
	soup = BeautifulSoup(f.read(), 'html.parser')
	f.close()
	currencies_table = soup.find("table", {"id": "currencies-all"})
	currencies_tbody = currencies_table.find('tbody')
	currency_rows = currencies_tbody.find_all("tr")
	for r in currency_rows:
		currency_short_name = r.find("td", {"class": "currency-name"}).find("span",{"class":"currency-symbol"}).find("a").text
		currency_name = r.find("td", {"class": "currency-name"}).find("a",{"class":"currency-name-container"}).text
		currency_market_cap = r.find("td", {"class": "market-cap"})['data-sort']
		currency_price = r.find("a",{"class": "price"}).text
		currency_volume = r.find("a",{"class": "volume"}).text
		currency_supply = r.find("td", {"class": "circulating-supply"})['data-sort']
		currency_change = r.find("td", {"class": "percent-change"})['data-sort']
		#print(currency_short_name)
		#print(currency_name)
		#print(currency_market_cap)
		# print(currency_price)
		# print(currency_volume)
		# print(currency_supply)
		# print(currency_change)
		df = df.append({
			'scrapping_time': scrapping_time,
			'short_name': currency_short_name,
			'name': currency_name,
			'market_cap': currency_market_cap,
			'price': currency_price,
			'volume': currency_volume,
			'supply': currency_supply,
			'24H_percent-change': currency_change
			}, ignore_index=True)
```
This code chunk creates a loop that for each html file finds each row which contains data for one currency. Another for loop 
is then nested within the loop that for each row extracts the currency's short name, name, market cap, price, volume, 
circulating supply, and percent change in the last 24 hours. It then appends each of these items into variables for a data 
frame that has already been initialized. 



#coinmarket_parser2.py

```
from bs4 import BeautifulSoup
import os
import glob
import pandas as pd
```
First the libraries required for this program are imported. 


```
for one_file_name in glob.glob("historical_html_files/*.html"):
	print("parsing " + one_file_name)
	f = open(one_file_name, "r")
	soup = BeautifulSoup(f.read(), 'html.parser')
	f.close()
	historical_table = soup.find("table")
	tbody = historical_table.find("tbody")
	rows = tbody.find_all("tr")
	for r in rows:
		tds = r.find_all("td")
		date = tds[0].text
		open_price = tds[1].text
		high = tds[2].text
		low = tds[3].text
		close_price = tds[4].text
		volume = tds[5].text
		market_cap = tds[6].text
		# print(tds)
		# print(date)
		# print(open_price)
		# print(high)
		# print(low)
		# print(cose_price)
		# print(volume)
		# print(market_cap)
		df = df.append({
			'date': date,
			'open_price': open_price,
			'high': high,
			'low': low,
			'close_price': close_price,
			'volume': volume,
			'market_cap': market_cap
			}, ignore_index=True)
```
This code chunk creates a for loop which loops through all of the html files containing the historical data for the top 500
currencies and finds each row within the tbody within the table. A for loop is again nested within the original loop which 
goes through each row and finds all the td's. Because the td's here do not contain product classes and are all for the same
currency, indices are used and .text is appended to only retrieve the data withing the quotations for each td. The variables
generated are then appended into a data frame that was initialized earlier. 



#coinmarket_analysis




  




 
